import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "const"
DAG_ID = "raw_from_s3_to_pg"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
LAYER = "raw"
SOURCE = "moex_ofz"
SCHEMA = "ods"
TARGET_TABLE = "dwh_bond"

# S3 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# Postgres Ñ‡ĞµÑ€ĞµĞ· DuckDB
PASSWORD = Variable.get("pg_password")

# ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ
LONG_DESCRIPTION = """
DAG Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ ÑÑ‹Ñ€Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ€Ğ³Ğ°Ğ¼ Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· S3 Ğ² Postgres (DuckDB Ñ‡ĞµÑ€ĞµĞ· HTTPFS).
"""
SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ² Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸"

# ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 8, 1, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    start_date = context['data_interval_start'].format('YYYY-MM-DD')
    end_date = context['data_interval_end'].format('YYYY-MM-DD')
    return start_date, end_date


def get_and_transfer_raw_data_to_ods_pg(**context):
    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")

    try:
        con = duckdb.connect()
        
        # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° S3 Ğ¸ PostgreSQL
        con.execute("INSTALL httpfs; LOAD httpfs;")
        con.execute("INSTALL postgres; LOAD postgres;")
        
        con.execute(f"""
            SET TIMEZONE='UTC';
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;
        """)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ² S3
        s3_path = f"s3://dev/{LAYER}/{SOURCE}/{start_date}/{start_date}_ofz.parquet"
        
        try:
            result = con.execute(f"SELECT COUNT(*) FROM '{s3_path}'").fetchone()
            row_count = result[0] if result else 0
            logging.info(f"Found {row_count} rows in {s3_path}")
            
            if row_count == 0:
                logging.warning(f"No data found in {s3_path}")
                return
                
        except Exception as e:
            logging.error(f"Cannot read file {s3_path}: {e}")
            raise

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ° Ğ´Ğ»Ñ PostgreSQL
        con.execute(f"""
            CREATE SECRET IF NOT EXISTS dwh_postgres (
                TYPE postgres,
                HOST 'postgres_dwh',
                PORT 5432,
                DATABASE 'postgres',
                USER 'postgres',
                PASSWORD '{PASSWORD}'
            );
        """)

        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº PostgreSQL
        con.execute("ATTACH '' AS dwh_postgres_db (TYPE postgres, SECRET dwh_postgres);")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            con.execute(f"DESCRIBE dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}")
            logging.info(f"Target table {SCHEMA}.{TARGET_TABLE} exists")
        except Exception as e:
            logging.error(f"Target table {SCHEMA}.{TARGET_TABLE} does not exist: {e}")
            raise

        # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· S3 Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² PostgreSQL
        insert_sql = f"""
        INSERT INTO dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
        (
            boardid,
            tradedate,
            shortname,
            secid,
            numtrades,
            value,
            low,
            high,
            close,
            legalcloseprice,
            accint,
            waprice,
            yieldclose,
            open,
            volume,
            marketprice2,
            marketprice3,
            mp2valtrd,
            marketprice3tradesvalue,
            matdate,
            duration,
            yieldatwap,
            couponpercent,
            couponvalue,
            lasttradedate,
            facevalue,
            currencyid,
            faceunit,
            tradingsession,
            trade_session_date
        )
        SELECT
            COALESCE(BOARDID, '') AS boardid,
            TRADEDATE::DATE AS tradedate,
            COALESCE(SHORTNAME, '') AS shortname,
            COALESCE(SECID, '') AS secid,
            COALESCE(NUMTRADES, 0) AS numtrades,
            COALESCE(VALUE, 0) AS value,
            COALESCE(LOW, 0) AS low,
            COALESCE(HIGH, 0) AS high,
            COALESCE(CLOSE, 0) AS close,
            COALESCE(LEGALCLOSEPRICE, 0) AS legalcloseprice,
            COALESCE(ACCINT, 0) AS accint,
            COALESCE(WAPRICE, 0) AS waprice,
            COALESCE(YIELDCLOSE, 0) AS yieldclose,
            COALESCE(OPEN, 0) AS open,
            COALESCE(VOLUME, 0) AS volume,
            COALESCE(MARKETPRICE2, 0) AS marketprice2,
            COALESCE(MARKETPRICE3, 0) AS marketprice3,
            COALESCE(MP2VALTRD, 0) AS mp2valtrd,
            COALESCE(MARKETPRICE3TRADESVALUE, 0) AS marketprice3tradesvalue,
            MATDATE::DATE AS matdate,
            COALESCE(DURATION, 0) AS duration,
            COALESCE(YIELDATWAP, 0) AS yieldatwap,
            COALESCE(COUPONPERCENT, 0) AS couponpercent,
            COALESCE(COUPONVALUE, 0) AS couponvalue,
            LASTTRADEDATE::DATE AS lasttradedate,
            COALESCE(FACEVALUE, 0) AS facevalue,
            COALESCE(CURRENCYID, '') AS currencyid,
            COALESCE(FACEUNIT, '') AS faceunit,
            COALESCE(TRADINGSESSION, '') AS tradingsession,
            COALESCE(TRADEDATE, CURRENT_DATE)::DATE AS trade_session_date
        FROM '{s3_path}'
        WHERE TRADEDATE IS NOT NULL
        """
        
        logging.info("Starting data insertion...")
        con.execute(insert_sql)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹
        inserted_count = con.execute(f"""
            SELECT COUNT(*) FROM dwh_postgres_db.{SCHEMA}.{TARGET_TABLE} 
            WHERE trade_session_date = '{start_date}'::DATE
        """).fetchone()[0]
        
        logging.info(f"âœ… Successfully inserted {inserted_count} records for date: {start_date}")
        
    except Exception as e:
        logging.error(f"âŒ Error in data transfer: {e}")
        raise
    finally:
        if 'con' in locals():
            con.close()


with DAG(
    dag_id=DAG_ID,
    schedule_interval='0 7 * * *',  # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ² 7:00, Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ DAG (6:00)
    default_args=default_args,
    tags=['s3', 'ods', 'pg'],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(
        task_id='start',
    )

    # Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ External Task Sensor
    def get_external_execution_date(execution_date, **context):
        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚Ñƒ Ğ¶Ğµ Ğ´Ğ°Ñ‚Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¸ Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ DAG'Ğ°
        return execution_date

    sensor_on_raw_layer = ExternalTaskSensor(
        task_id='sensor_on_raw_layer',
        external_dag_id='raw_ofz_from_moex_to_s3',
        external_task_id='end',
        execution_date_fn=get_external_execution_date,
        allowed_states=['success'],
        mode='poke',  # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¾ Ñ 'reschedule' Ğ½Ğ° 'poke'
        timeout=7200,  # 2 Ñ‡Ğ°ÑĞ°
        poke_interval=60,  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ
    )

    transfer_task = PythonOperator(
        task_id='get_and_transfer_raw_data_to_ods_pg',
        python_callable=get_and_transfer_raw_data_to_ods_pg,
    )

    end = EmptyOperator(
        task_id='end',
    )

    start >> sensor_on_raw_layer >> transfer_task >> end