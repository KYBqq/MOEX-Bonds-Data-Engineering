import logging
import requests
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
OWNER = "const"
DAG_ID = "unified_moex_pipeline"

# –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
LAYER = "raw"
SOURCE = "moex_ofz"
SCHEMA = "ods"
TARGET_TABLE = "dwh_bond"

# S3 –¥–æ—Å—Ç—É–ø
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# Postgres —á–µ—Ä–µ–∑ DuckDB
PASSWORD = Variable.get("pg_password")

# –û–ø–∏—Å–∞–Ω–∏—è
SHORT_DESCRIPTION = "–ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –∑–∞–≥—Ä—É–∑–∫–∞ OFZ —Å MOEX –≤ S3 –∏ –∑–∞—Ç–µ–º –≤ PostgreSQL"

default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 8, 1, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}

def get_dates(**context) -> tuple[str, str]:
    exec_date = context["execution_date"]
    prev_date = (exec_date - pendulum.duration(days=1)).format("YYYY-MM-DD")
    return prev_date, prev_date

def fetch_and_store_ofz(**context):
    start_date, end_date = get_dates(**context)
    logging.info(f"Start loading OFZ from {start_date} to {end_date}")

    # 1) –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±–ª–∏–≥–∞—Ü–∏–π —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π API endpoint
    url_list = (
        "https://iss.moex.com/iss/engines/stock/markets/bonds/securities.json"
        "?iss.meta=off"
    )
    
    try:
        resp = requests.get(url_list, timeout=30)
        resp.raise_for_status()
        data = resp.json().get("securities")
        if not data:
            logging.error("–ù–µ—Ç –±–ª–æ–∫–∞ 'securities' –≤ –æ—Ç–≤–µ—Ç–µ")
            return
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –±—É–º–∞–≥: {e}")
        return

    sec_cols = data["columns"]
    sec_rows = data["data"]

    logging.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(sec_rows)} –æ–±–ª–∏–≥–∞—Ü–∏–π –≤—Å–µ–≥–æ")

    # –ù–∞–π–¥–µ–º –∏–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ SECID
    secid_idx = sec_cols.index("SECID")

    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±–ª–∏–≥–∞—Ü–∏–∏
    secids = []
    for row in sec_rows:
        if row and len(row) > secid_idx:
            secid = row[secid_idx] if row[secid_idx] else ""
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –æ–±–ª–∏–≥–∞—Ü–∏–∏ —Å –Ω–µ–ø—É—Å—Ç—ã–º SECID
            if secid:
                secids.append(secid)

    logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(secids)} –æ–±–ª–∏–≥–∞—Ü–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 100 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    secids = secids[:100]
    logging.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {len(secids)} –æ–±–ª–∏–≥–∞—Ü–∏–π")

    # 2) –°–∫–∞—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ –∫–∞–∂–¥–æ–º—É secid
    all_rows = []
    cols = None
    successful_downloads = 0
    
    for sec in secids:
        url_hist = (
            f"https://iss.moex.com/iss/history/engines/stock/markets/bonds/"
            f"boards/TQOB/securities/{sec}.json"
            f"?from={start_date}&till={end_date}&iss.meta=off"
        )
        
        try:
            r = requests.get(url_hist, timeout=30)
            if not r.ok:
                logging.warning(f"–û—à–∏–±–∫–∞ {r.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {sec}")
                continue

            hist_data = r.json().get("history", {})
            cols_raw = hist_data.get("columns", [])
            if not cols:
                cols = cols_raw[:]  # –ö–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
            
            rows_count = 0
            for row in hist_data.get("data", []):
                if row:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∫–æ–ª–æ–Ω–∫–∞ SECID –≤ –¥–∞–Ω–Ω—ã—Ö
                    if "SECID" in cols:
                        # –ï—Å–ª–∏ SECID —É–∂–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
                        all_rows.append(row)
                    else:
                        # –ï—Å–ª–∏ SECID –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ sec –≤ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏
                        all_rows.append(row + [sec])
                    rows_count += 1
            
            if rows_count > 0:
                successful_downloads += 1
                
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {sec}: {e}")
            continue

    if not all_rows:
        logging.warning(f"–ù–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")
        logging.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {successful_downloads} –∏–∑ {len(secids)} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
        return

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É SECID —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—ë –µ—â—ë –Ω–µ—Ç
    if "SECID" not in cols:
        cols.append("SECID")
        logging.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ SECID")
    else:
        logging.info(f"–ö–æ–ª–æ–Ω–∫–∞ SECID —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ API response")
    
    logging.info(f"–ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_rows)} –∑–∞–ø–∏—Å–µ–π")

    # 3) –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ S3 —á–µ—Ä–µ–∑ DuckDB
    try:
        con = duckdb.connect()
        con.execute("INSTALL httpfs; LOAD httpfs;")
        con.execute(f"""
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;
        """)

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        col_definitions = []
        for i, col in enumerate(cols):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
            sample_val = all_rows[0][i] if all_rows and len(all_rows[0]) > i else None
            if isinstance(sample_val, (int, float)) and sample_val is not None:
                col_type = "DOUBLE" if isinstance(sample_val, float) else "BIGINT"
            else:
                col_type = "VARCHAR"
            col_definitions.append(f'"{col}" {col_type}')
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
        create_table_sql = f"""
        CREATE TABLE ofz_temp ({', '.join(col_definitions)})
        """
        con.execute(create_table_sql)
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Ä—Ü–∏—è–º–∏
        batch_size = 1000
        for i in range(0, len(all_rows), batch_size):
            batch = all_rows[i:i + batch_size]
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º VALUES –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            values_list = []
            for row in batch:
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NULL
                escaped_row = []
                for val in row:
                    if val is None:
                        escaped_row.append("NULL")
                    elif isinstance(val, str):
                        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏
                        escaped_val = val.replace("'", "''")
                        escaped_row.append(f"'{escaped_val}'")
                    else:
                        escaped_row.append(str(val))
                values_list.append(f"({', '.join(escaped_row)})")
            
            values_str = ', '.join(values_list)
            insert_sql = f"INSERT INTO ofz_temp VALUES {values_str}"
            con.execute(insert_sql)

        # –ö–æ–ø–∏—Ä—É–µ–º –≤ Parquet –Ω–∞ S3
        s3_path = f"s3://dev/{LAYER}/{SOURCE}/{start_date}/{start_date}_ofz.parquet"
        con.execute(f"""
            COPY (SELECT * FROM ofz_temp)
            TO '{s3_path}'
            (FORMAT 'parquet', COMPRESSION 'gzip');
        """)
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        con.execute("DROP TABLE ofz_temp")
        con.close()
        logging.info(f"‚úÖ Bond data saved to {s3_path}")
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ S3: {e}")
        if 'con' in locals():
            try:
                con.execute("DROP TABLE IF EXISTS ofz_temp")
                con.close()
            except:
                pass
        raise

def transfer_s3_to_pg(**context):
    start_date, end_date = get_dates(**context)
    logging.info(f"üíª Start load for dates: {start_date}/{end_date}")

    try:
        con = duckdb.connect()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ S3 –∏ PostgreSQL
        con.execute("INSTALL httpfs; LOAD httpfs;")
        con.execute("INSTALL postgres; LOAD postgres;")
        
        con.execute(f"""
            SET TIMEZONE='UTC';
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;
        """)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ S3
        s3_path = f"s3://dev/{LAYER}/{SOURCE}/{start_date}/{start_date}_ofz.parquet"
        
        try:
            result = con.execute(f"SELECT COUNT(*) FROM '{s3_path}'").fetchone()
            row_count = result[0] if result else 0
            logging.info(f"Found {row_count} rows in {s3_path}")
            
            if row_count == 0:
                logging.warning(f"No data found in {s3_path}")
                return
                
        except Exception as e:
            logging.error(f"Cannot read file {s3_path}: {e}")
            raise

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–∫—Ä–µ—Ç–∞ –¥–ª—è PostgreSQL
        con.execute(f"""
            CREATE SECRET IF NOT EXISTS dwh_postgres (
                TYPE postgres,
                HOST 'postgres_dwh',
                PORT 5432,
                DATABASE 'postgres',
                USER 'postgres',
                PASSWORD '{PASSWORD}'
            );
        """)

        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL
        con.execute("ATTACH '' AS dwh_postgres_db (TYPE postgres, SECRET dwh_postgres);")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
        try:
            con.execute(f"DESCRIBE dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}")
            logging.info(f"Target table {SCHEMA}.{TARGET_TABLE} exists")
        except Exception as e:
            logging.error(f"Target table {SCHEMA}.{TARGET_TABLE} does not exist: {e}")
            raise

        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ S3 –∏ –≤—Å—Ç–∞–≤–ª—è–µ–º –≤ PostgreSQL
        insert_sql = f"""
        INSERT INTO dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
        (
            boardid,
            tradedate,
            shortname,
            secid,
            numtrades,
            value,
            low,
            high,
            close,
            legalcloseprice,
            accint,
            waprice,
            yieldclose,
            open,
            volume,
            marketprice2,
            marketprice3,
            mp2valtrd,
            marketprice3tradesvalue,
            matdate,
            duration,
            yieldatwap,
            couponpercent,
            couponvalue,
            lasttradedate,
            facevalue,
            currencyid,
            faceunit,
            tradingsession,
            trade_session_date
        )
        SELECT
            COALESCE(BOARDID, '') AS boardid,
            TRADEDATE::DATE AS tradedate,
            COALESCE(SHORTNAME, '') AS shortname,
            COALESCE(SECID, '') AS secid,
            COALESCE(NUMTRADES, 0) AS numtrades,
            COALESCE(VALUE, 0) AS value,
            COALESCE(LOW, 0) AS low,
            COALESCE(HIGH, 0) AS high,
            COALESCE(CLOSE, 0) AS close,
            COALESCE(LEGALCLOSEPRICE, 0) AS legalcloseprice,
            COALESCE(ACCINT, 0) AS accint,
            COALESCE(WAPRICE, 0) AS waprice,
            COALESCE(YIELDCLOSE, 0) AS yieldclose,
            COALESCE(OPEN, 0) AS open,
            COALESCE(VOLUME, 0) AS volume,
            COALESCE(MARKETPRICE2, 0) AS marketprice2,
            COALESCE(MARKETPRICE3, 0) AS marketprice3,
            COALESCE(MP2VALTRD, 0) AS mp2valtrd,
            COALESCE(MARKETPRICE3TRADESVALUE, 0) AS marketprice3tradesvalue,
            MATDATE::DATE AS matdate,
            COALESCE(DURATION, 0) AS duration,
            COALESCE(YIELDATWAP, 0) AS yieldatwap,
            COALESCE(COUPONPERCENT, 0) AS couponpercent,
            COALESCE(COUPONVALUE, 0) AS couponvalue,
            LASTTRADEDATE::DATE AS lasttradedate,
            COALESCE(FACEVALUE, 0) AS facevalue,
            COALESCE(CURRENCYID, '') AS currencyid,
            COALESCE(FACEUNIT, '') AS faceunit,
            COALESCE(TRADINGSESSION, '') AS tradingsession,
            '{start_date}'::DATE AS trade_session_date
        FROM '{s3_path}'
        WHERE TRADEDATE IS NOT NULL
        """
        
        logging.info("Starting data insertion...")
        con.execute(insert_sql)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        inserted_count = con.execute(f"""
            SELECT COUNT(*) FROM dwh_postgres_db.{SCHEMA}.{TARGET_TABLE} 
            WHERE trade_session_date = '{start_date}'::DATE
        """).fetchone()[0]
        
        logging.info(f"‚úÖ Successfully inserted {inserted_count} records for date: {start_date}")
        
    except Exception as e:
        logging.error(f"‚ùå Error in data transfer: {e}")
        raise
    finally:
        if 'con' in locals():
            con.close()

with DAG(
    dag_id=DAG_ID,
    schedule_interval='0 7 * * *',  # –ó–∞–ø—É—Å–∫ –≤ 7:00
    default_args=default_args,
    tags=['moex', 'unified', 'pipeline'],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:

    start = EmptyOperator(task_id='start')

    # –ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å MOEX –≤ S3
    fetch_and_store_ofz_task = PythonOperator(
        task_id='fetch_and_store_ofz',
        python_callable=fetch_and_store_ofz,
    )

    # –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø: –ø–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏–∑ S3 –≤ PostgreSQL
    transfer_s3_to_pg_task = PythonOperator(
        task_id='transfer_s3_to_pg',
        python_callable=transfer_s3_to_pg,
    )

    end = EmptyOperator(task_id='end')

    # –¶–µ–ø–æ—á–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    start >> fetch_and_store_ofz_task >> transfer_s3_to_pg_task >> end

