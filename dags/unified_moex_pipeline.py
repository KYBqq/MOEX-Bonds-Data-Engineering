import logging
import requests
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "const"
DAG_ID = "unified_moex_pipeline"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
LAYER = "raw"
SOURCE = "moex_ofz"
SCHEMA = "ods"
TARGET_TABLE = "dwh_bond"

# S3 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# Postgres Ñ‡ĞµÑ€ĞµĞ· DuckDB
PASSWORD = Variable.get("pg_password")

# ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ
SHORT_DESCRIPTION = "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½: Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° OFZ Ñ MOEX Ğ² S3 Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ² PostgreSQL"

default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 8, 1, tz="Europe/Moscow"),
    "catchup": True,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}

def get_dates(**context) -> tuple[str, str]:
    exec_date = context["execution_date"]
    prev_date = (exec_date - pendulum.duration(days=1)).format("YYYY-MM-DD")
    return prev_date, prev_date

def fetch_and_store_ofz(**context):
    start_date, end_date = get_dates(**context)
    logging.info(f"Start loading OFZ from {start_date} to {end_date}")

    # 1) ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ API endpoint
    url_list = (
        "https://iss.moex.com/iss/engines/stock/markets/bonds/securities.json"
        "?iss.meta=off"
    )
    
    try:
        resp = requests.get(url_list, timeout=30)
        resp.raise_for_status()
        data = resp.json().get("securities")
        if not data:
            logging.error("ĞĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ° 'securities' Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ")
            return
    except Exception as e:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¸ÑĞºĞ° Ğ±ÑƒĞ¼Ğ°Ğ³: {e}")
        return

    sec_cols = data["columns"]
    sec_rows = data["data"]

    logging.info(f"ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾ {len(sec_rows)} Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¹ Ğ²ÑĞµĞ³Ğ¾")

    # ĞĞ°Ğ¹Ğ´ĞµĞ¼ Ğ¸Ğ½Ğ´ĞµĞºÑ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ SECID
    secid_idx = sec_cols.index("SECID")

    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸
    secids = []
    for row in sec_rows:
        if row and len(row) > secid_idx:
            secid = row[secid_idx] if row[secid_idx] else ""
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²ÑĞµ Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ĞµĞ¿ÑƒÑÑ‚Ñ‹Ğ¼ SECID
            if secid:
                secids.append(secid)

    logging.info(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(secids)} Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸")

    # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¾ 100 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    secids = secids[:100]
    logging.info(f"ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ´Ğ¾ {len(secids)} Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¹")

    # 2) Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ secid
    all_rows = []
    cols = None
    successful_downloads = 0
    
    for sec in secids:
        url_hist = (
            f"https://iss.moex.com/iss/history/engines/stock/markets/bonds/"
            f"boards/TQOB/securities/{sec}.json"
            f"?from={start_date}&till={end_date}&iss.meta=off"
        )
        
        try:
            r = requests.get(url_hist, timeout=30)
            if not r.ok:
                logging.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° {r.status_code} Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ {sec}")
                continue

            hist_data = r.json().get("history", {})
            cols_raw = hist_data.get("columns", [])
            if not cols:
                cols = cols_raw[:]  # ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
            
            rows_count = 0
            for row in hist_data.get("data", []):
                if row:
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ ÑƒĞ¶Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° SECID Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                    if "SECID" in cols:
                        # Ğ•ÑĞ»Ğ¸ SECID ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞµÑ‘
                        all_rows.append(row)
                    else:
                        # Ğ•ÑĞ»Ğ¸ SECID Ğ½ĞµÑ‚, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ sec Ğ² ĞºĞ¾Ğ½ĞµÑ† ÑÑ‚Ñ€Ğ¾ĞºĞ¸
                        all_rows.append(row + [sec])
                    rows_count += 1
            
            if rows_count > 0:
                successful_downloads += 1
                
        except Exception as e:
            logging.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ {sec}: {e}")
            continue

    if not all_rows:
        logging.warning(f"ĞĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ {start_date} - {end_date}")
        logging.info(f"Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ {successful_downloads} Ğ¸Ğ· {len(secids)} Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²")
        return

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ SECID Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ ĞµÑ‘ ĞµÑ‰Ñ‘ Ğ½ĞµÑ‚
    if "SECID" not in cols:
        cols.append("SECID")
        logging.info(f"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° SECID")
    else:
        logging.info(f"ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° SECID ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² API response")
    
    logging.info(f"Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(all_rows)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")

    # 3) Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² S3 Ñ‡ĞµÑ€ĞµĞ· DuckDB
    try:
        con = duckdb.connect()
        con.execute("INSTALL httpfs; LOAD httpfs;")
        con.execute(f"""
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;
        """)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        col_definitions = []
        for i, col in enumerate(cols):
            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
            sample_val = all_rows[0][i] if all_rows and len(all_rows[0]) > i else None
            if isinstance(sample_val, (int, float)) and sample_val is not None:
                col_type = "DOUBLE" if isinstance(sample_val, float) else "BIGINT"
            else:
                col_type = "VARCHAR"
            col_definitions.append(f'"{col}" {col_type}')
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ
        create_table_sql = f"""
        CREATE TABLE ofz_temp ({', '.join(col_definitions)})
        """
        con.execute(create_table_sql)
        
        # Ğ’ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ€Ñ†Ğ¸ÑĞ¼Ğ¸
        batch_size = 1000
        for i in range(0, len(all_rows), batch_size):
            batch = all_rows[i:i + batch_size]
            
            # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ VALUES Ğ´Ğ»Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸
            values_list = []
            for row in batch:
                # Ğ­ĞºÑ€Ğ°Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ NULL
                escaped_row = []
                for val in row:
                    if val is None:
                        escaped_row.append("NULL")
                    elif isinstance(val, str):
                        # Ğ­ĞºÑ€Ğ°Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ´Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸
                        escaped_val = val.replace("'", "''")
                        escaped_row.append(f"'{escaped_val}'")
                    else:
                        escaped_row.append(str(val))
                values_list.append(f"({', '.join(escaped_row)})")
            
            values_str = ', '.join(values_list)
            insert_sql = f"INSERT INTO ofz_temp VALUES {values_str}"
            con.execute(insert_sql)

        # ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Parquet Ğ½Ğ° S3
        s3_path = f"s3://dev/{LAYER}/{SOURCE}/{start_date}/{start_date}_ofz.parquet"
        con.execute(f"""
            COPY (SELECT * FROM ofz_temp)
            TO '{s3_path}'
            (FORMAT 'parquet', COMPRESSION 'gzip');
        """)
        
        # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ
        con.execute("DROP TABLE ofz_temp")
        con.close()
        logging.info(f"âœ… Bond data saved to {s3_path}")
        
    except Exception as e:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ² S3: {e}")
        if 'con' in locals():
            try:
                con.execute("DROP TABLE IF EXISTS ofz_temp")
                con.close()
            except:
                pass
        raise

def transfer_s3_to_pg(**context):
    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")

    try:
        con = duckdb.connect()
        
        # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° S3 Ğ¸ PostgreSQL
        con.execute("INSTALL httpfs; LOAD httpfs;")
        con.execute("INSTALL postgres; LOAD postgres;")
        
        con.execute(f"""
            SET TIMEZONE='UTC';
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;
        """)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ² S3
        s3_path = f"s3://dev/{LAYER}/{SOURCE}/{start_date}/{start_date}_ofz.parquet"
        
        try:
            result = con.execute(f"SELECT COUNT(*) FROM '{s3_path}'").fetchone()
            row_count = result[0] if result else 0
            logging.info(f"Found {row_count} rows in {s3_path}")
            
            if row_count == 0:
                logging.warning(f"No data found in {s3_path}")
                return
                
        except Exception as e:
            logging.error(f"Cannot read file {s3_path}: {e}")
            raise

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ° Ğ´Ğ»Ñ PostgreSQL
        con.execute(f"""
            CREATE SECRET IF NOT EXISTS dwh_postgres (
                TYPE postgres,
                HOST 'postgres_dwh',
                PORT 5432,
                DATABASE 'postgres',
                USER 'postgres',
                PASSWORD '{PASSWORD}'
            );
        """)

        # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº PostgreSQL
        con.execute("ATTACH '' AS dwh_postgres_db (TYPE postgres, SECRET dwh_postgres);")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
        try:
            con.execute(f"DESCRIBE dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}")
            logging.info(f"Target table {SCHEMA}.{TARGET_TABLE} exists")
        except Exception as e:
            logging.error(f"Target table {SCHEMA}.{TARGET_TABLE} does not exist: {e}")
            raise

        # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· S3 Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² PostgreSQL
        insert_sql = f"""
        INSERT INTO dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
        (
            boardid,
            tradedate,
            shortname,
            secid,
            numtrades,
            value,
            low,
            high,
            close,
            legalcloseprice,
            accint,
            waprice,
            yieldclose,
            open,
            volume,
            marketprice2,
            marketprice3,
            mp2valtrd,
            marketprice3tradesvalue,
            matdate,
            duration,
            yieldatwap,
            couponpercent,
            couponvalue,
            lasttradedate,
            facevalue,
            currencyid,
            faceunit,
            tradingsession,
            trade_session_date
        )
        SELECT
            COALESCE(BOARDID, '') AS boardid,
            TRADEDATE::DATE AS tradedate,
            COALESCE(SHORTNAME, '') AS shortname,
            COALESCE(SECID, '') AS secid,
            COALESCE(NUMTRADES, 0) AS numtrades,
            COALESCE(VALUE, 0) AS value,
            COALESCE(LOW, 0) AS low,
            COALESCE(HIGH, 0) AS high,
            COALESCE(CLOSE, 0) AS close,
            COALESCE(LEGALCLOSEPRICE, 0) AS legalcloseprice,
            COALESCE(ACCINT, 0) AS accint,
            COALESCE(WAPRICE, 0) AS waprice,
            COALESCE(YIELDCLOSE, 0) AS yieldclose,
            COALESCE(OPEN, 0) AS open,
            COALESCE(VOLUME, 0) AS volume,
            COALESCE(MARKETPRICE2, 0) AS marketprice2,
            COALESCE(MARKETPRICE3, 0) AS marketprice3,
            COALESCE(MP2VALTRD, 0) AS mp2valtrd,
            COALESCE(MARKETPRICE3TRADESVALUE, 0) AS marketprice3tradesvalue,
            MATDATE::DATE AS matdate,
            COALESCE(DURATION, 0) AS duration,
            COALESCE(YIELDATWAP, 0) AS yieldatwap,
            COALESCE(COUPONPERCENT, 0) AS couponpercent,
            COALESCE(COUPONVALUE, 0) AS couponvalue,
            LASTTRADEDATE::DATE AS lasttradedate,
            COALESCE(FACEVALUE, 0) AS facevalue,
            COALESCE(CURRENCYID, '') AS currencyid,
            COALESCE(FACEUNIT, '') AS faceunit,
            COALESCE(TRADINGSESSION, '') AS tradingsession,
            '{start_date}'::DATE AS trade_session_date
        FROM '{s3_path}'
        WHERE TRADEDATE IS NOT NULL
        """
        
        logging.info("Starting data insertion...")
        con.execute(insert_sql)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹
        inserted_count = con.execute(f"""
            SELECT COUNT(*) FROM dwh_postgres_db.{SCHEMA}.{TARGET_TABLE} 
            WHERE trade_session_date = '{start_date}'::DATE
        """).fetchone()[0]
        
        logging.info(f"âœ… Successfully inserted {inserted_count} records for date: {start_date}")
        
    except Exception as e:
        logging.error(f"âŒ Error in data transfer: {e}")
        raise
    finally:
        if 'con' in locals():
            con.close()

with DAG(
    dag_id=DAG_ID,
    schedule_interval='0 7 * * *',  # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ² 7:00
    default_args=default_args,
    tags=['moex', 'unified', 'pipeline'],
    description=SHORT_DESCRIPTION,
    concurrency=1,
    max_active_tasks=1,
    max_active_runs=1,
) as dag:

    start = EmptyOperator(task_id='start')

    # ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿: Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ MOEX Ğ² S3
    fetch_and_store_ofz_task = PythonOperator(
        task_id='fetch_and_store_ofz',
        python_callable=fetch_and_store_ofz,
    )

    # Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿: Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· S3 Ğ² PostgreSQL
    transfer_s3_to_pg_task = PythonOperator(
        task_id='transfer_s3_to_pg',
        python_callable=transfer_s3_to_pg,
    )

    end = EmptyOperator(task_id='end')

    # Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
    start >> fetch_and_store_ofz_task >> transfer_s3_to_pg_task >> end

